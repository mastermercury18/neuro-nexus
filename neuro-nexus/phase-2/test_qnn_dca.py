# -*- coding: utf-8 -*-
"""test_qnn_dca

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qj1AjAAnWv2knoxC1E_ytNhZ97MpvPHj
"""

!pip install qiskit

from qiskit.circuit import QuantumCircuit

import numpy as np
import matplotlib.pyplot as plt
#plt.style.use('light_background') # optional

!pip install qiskit_aer

from qiskit_aer import AerSimulator
from qiskit import QuantumCircuit, transpile

params = list(np.random.uniform(0, np.pi, size=400))  # 20 qubits
print(params)

print(big_statevectors)

phenotypes = ["LOW", "LOW", "HIGH", "LOW", "LOW", "HIGH", "HIGH", "LOW", "HIGH", "LOW", "LOW", "LOW", "HIGH", 'HIGH', 'LOW', "HIGH", 'HIGH', 'LOW'
, 'HIGH', 'LOW' ]
print(phenotypes)

def build_circuit(params1_vals, params2_vals, grid):
  dim = 4
  num_qubits = 4
  # create a 20x20 nested list for the snp genotypes
  qc = QuantumCircuit(dim)

  for i in range(dim):
    for j in range(dim):
      theta = get_theta(grid[i][j])
      if j == 0:
        qc.h(i)
      if grid[i][j] == -1:
        qc.id(i)
      elif j % 2 == 0:
        qc.rx(theta, i)
      elif j % 2 == 1:
        qc.rz(theta, i)

  for qubit in range(num_qubits):
      qc.ry(params1_vals[qubit], qubit)

  qc.cx(3, 0)

  for qubit in range(num_qubits - 1):
      qc.cx(qubit, qubit + 1)

  for qubit in range(num_qubits):
      qc.ry(params2_vals[qubit], qubit)

  for qubit in reversed(range(num_qubits)):
      qc.cx(qubit - 1, qubit)
  qc.measure_all()
  return qc



def get_theta(n):
  return (n+1) * np.pi/3

params = list(np.random.uniform(0, np.pi, size=400))  # 20 qubits
print(params)

from scipy.optimize import minimize

from qiskit.quantum_info import Statevector

statevectors_list = []

# Use the statevector simulator backend for this demonstration.
def cost_function(x):
    num_qubits = 4
    """
    Cost function for the variational algorithm.
    x is a vector of length 40 (first 20 for params1, next 20 for params2).

    For demonstration, we simulate the circuit and compute the distance
    between the resulting statevector and a target state.
    (In your application, you would design a cost function that
     encourages the circuit to output feature vectors that improve your ML task.)
    """

    # Split parameters: first half for first RY layer, second half for second RY layer.
    params1_vals = x[:num_qubits]
    params2_vals = x[num_qubits:]

    # Build the circuit with these parameter values.
    circuit = build_circuit(params1_vals, params2_vals, grid)

    # Execute the circuit to obtain the statevector.
    #job = execute(circuit, backend)

    qc_aer = transpile(circuit, backend=AerSimulator())
    simulator_aer = AerSimulator()
    results = simulator_aer.run(qc_aer, shots=1024).result()
    counts=results.get_counts()
    #statevector = results.get_statevector(circuit)
    circuit.remove_final_measurements()
    statevector = Statevector(circuit)
    statevectors_list.append(statevector)
    #print(statevector)

    # Dummy target: we choose the all-|0> state (i.e. statevector with 1 in the first entry).
    # In practice, replace this with an objective that measures how "good" your feature vectors are.
    target = np.zeros(2**num_qubits, dtype=complex)
    target[0] = 1.0
    # Compute the L2 norm (Euclidean distance) between the statevector and the target.
    cost = np.linalg.norm(statevector - target)
    return cost

num_qubits = 4
big_statevectors = []
#for row in big_list:
  # Given list of 372 elements (example values from 1 to 372)
big_list = [
    [2, 2, 2, 1, 2, 2, 0, 0, 0, 0, 0, 2],
    [0, 0, 0, 1, 1, 1, 2, 0, 2, 0, 1, 0],
    [0, 2, 0, 2, 2, 1, 1, 1, 1, 1, 0, 1],
    [2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],
    [2, 2, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0],
    [0, 0, 1, 1, 2, 1, 1, 1, 0, 2, 2, 0],
    [2, 0, 1, 2, 1, 1, 1, 1, 0, 2, 0, 2],
    [0, 0, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1],
    [0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 2, 1],
    [2, 2, 0, 0, 1, 2, 1, 2, 2, 0, 1, 2],
    [2, 2, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0],
    [2, 0, 0, 0, 1, 1, 1, 2, 2, 0, 2, 1],
    [2, 2, 1, 0, 1, 2, 0, 1, 2, 2, 1, 2],
    [0, 2, 1, 0, 1, 2, 1, 1, 2, 2, 1, 0],
    [2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0],
    [2, 1, 2, 0, 1, 0, 0, 2, 0, 2, 0, 2],
    [0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 2, 1],
    [2, 2, 2, 0, 1, 2, 1, 1, 0, 0, 0, 0],
    [2, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2],
    [0, 1, 1, 1, 2, 2, 0, 2, 2, 0, 1, 0]
]

for elements in big_list:
# Initialize a 20x20 matrix filled with -1
  matrix = np.full((4, 4), -1)

  # Fill the matrix with the given elements
  matrix.flat[:len(elements)] = elements

  # Convert to a list of lists
  grid = matrix.tolist()

  # Print the matrix (optional)
  for row in grid:
      print(row)

  # Initial guess: random parameters between 0 and pi.
  x0 = np.random.uniform(0, np.pi, size=2 * num_qubits)

  # Use a classical optimizer (COBYLA in this example) to minimize the cost function.
  result = minimize(cost_function, x0, method='COBYLA')

  print("Optimized parameters:")
  print(result.x)
  print("Final cost:", result.fun)
  print('Last Statevector:')
  print(statevectors_list[len(statevectors_list)-2])
  big_statevectors.append(statevectors_list[len(statevectors_list)-2])
  statevectors_list = []

  # ----- Extract Optimized Feature Vectors -----
  # With the optimized parameters, rebuild and simulate the circuit.
  opt_params1 = result.x[:num_qubits]
  opt_params2 = result.x[num_qubits:]
  opt_circuit = build_circuit(opt_params1, opt_params2, grid)

  # Here you can choose how to extract your feature vectors.
  # For instance, you might measure expectation values of certain observables
  # or simply use the statevector amplitudes (or a function of them) as features.
  qc_aer = transpile(opt_circuit, backend=AerSimulator())
  simulator_aer = AerSimulator()
  results = simulator_aer.run(qc_aer, shots=1024).result()
  counts=results.get_counts()
  final_state = qc_aer.remove_final_measurements()
  statevector = Statevector(qc_aer)
  print("Final statevector (can be processed to yield feature vectors):")
  print(final_state)

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Assume you have already imported Qiskit's Statevector class and your statevectors are available.
# from qiskit.quantum_info import Statevector
# For example, your statevectors list might look like:
# statevectors = [Statevector(...), Statevector(...), ...]  # 20 elements

# Your provided phenotype list for each patient:
phenotypes = ['LOW', 'LOW', 'HIGH', 'LOW', 'LOW', 'HIGH', 'HIGH', 'LOW', 'HIGH', 'LOW',
              'LOW', 'LOW', 'HIGH', 'HIGH', 'LOW', 'HIGH', 'HIGH', 'LOW', 'HIGH', 'LOW']

# --- Step 1: Convert statevectors into feature vectors ---
def statevector_to_features(sv):
    """
    Convert a Qiskit statevector into a feature vector by computing
    the squared magnitude (probability) of each amplitude.
    """
    # Assuming sv.data is a numpy array of complex amplitudes
    return np.abs(sv.data)**2

# Suppose your statevectors list is defined as "statevectors".
# Here we create a dummy list for illustration.
# In practice, replace this with your actual statevectors list.
from qiskit.quantum_info import Statevector
# For demonstration, we create 20 random statevectors for a 4-qubit circuit.
num_qubits = 4
statevectors = big_statevectors  # Replace with your actual list

# Convert each statevector to a feature vector (length 16)
features = np.array([statevector_to_features(sv) for sv in statevectors])  # shape: (20, 16)

# --- Step 2: Encode phenotype labels ---
# Convert 'LOW' to 0 and 'HIGH' to 1
labels = np.array([0 if pheno == 'LOW' else 1 for pheno in phenotypes])

# --- Step 3: Split into training (first 14) and test sets (last 6) ---
X_train = features[:14]  # shape: (14, 16)
y_train = labels[:14]    # shape: (14,)
X_test  = features[14:]  # shape: (6, 16)
y_test  = labels[14:]    # shape: (6,)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # shape: (14, 1)
X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)   # shape: (6, 1)

# --- Step 4: Define the Neural Network ---
class PhenotypeClassifier(nn.Module):
    def __init__(self, input_dim=16, hidden_dim=32, output_dim=1):
        super(PhenotypeClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification

    def forward(self, x, return_features=False):
        x1 = self.relu(self.fc1(x))
        x2 = self.fc2(x1)
        out = self.sigmoid(x2)
        if return_features:
            return x1  # return hidden features before output layer
        else:
            return out

model = PhenotypeClassifier()

# Loss function and optimizer
criterion = nn.BCELoss()  # Binary cross entropy for classification
optimizer = optim.Adam(model.parameters(), lr=0.01)

# --- Training Loop ---
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')

print("Training complete.")

# --- Evaluate on Test Data ---
model.eval()
with torch.no_grad():
    test_outputs = model(X_test_tensor)
    predicted = (test_outputs > 0.5).float()
    accuracy = (predicted.eq(y_test_tensor).sum().item()) / y_test_tensor.shape[0]
    print("\nTest Accuracy:", accuracy)

    # Convert predicted values to phenotype strings
    predicted_labels = ['HIGH' if pred.item() == 1.0 else 'LOW' for pred in predicted]
    print("Predicted phenotypes for the last 6 patients:")
    for i, label in enumerate(predicted_labels, start=15):
        print(f"Patient {i+1}: {label}")

pip install torch

pip install scikit-learn

from sklearn.decomposition import PCA, FastICA
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Extract latent features from the hidden layer using the updated model
with torch.no_grad():
    feats_qnn = model(X_test_tensor, return_features=True).numpy()  # uses the new argument

# Apply PCA to reduce to 2D
pca = PCA(n_components=2)
reduced_qnn = pca.fit_transform(feats_qnn)

# Plotting the latent space colored by true labels
plt.figure(figsize=(8, 6))
plt.scatter(reduced_qnn[:, 0], reduced_qnn[:, 1], c=y_test, cmap='coolwarm', s=50, edgecolor='k')
plt.title("QNN Latent Space (PCA)")
plt.xlabel("PC 1")
plt.ylabel("PC 2")
plt.colorbar(label='Phenotype (0=LOW, 1=HIGH)')
plt.show()

# --- Step 1: Define Classical Neural Network ---
class ClassicalClassifier(nn.Module):
    def __init__(self, input_dim=16, hidden_dim=32, output_dim=1):
        super(ClassicalClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, return_features=False):
        x1 = self.relu(self.fc1(x))
        x2 = self.fc2(x1)
        out = self.sigmoid(x2)
        if return_features:
            return x1  # return hidden layer activations
        else:
            return out

# --- Step 2: Train Classical Neural Network ---
model_classical = ClassicalClassifier()

criterion_cnn = nn.BCELoss()
optimizer_cnn = optim.Adam(model_classical.parameters(), lr=0.01)

# Train loop
for epoch in range(num_epochs):
    model_classical.train()
    optimizer_cnn.zero_grad()
    outputs = model_classical(X_train_tensor)
    loss = criterion_cnn(outputs, y_train_tensor)
    loss.backward()
    optimizer_cnn.step()

    if (epoch + 1) % 10 == 0:
        print(f'[Classical NN] Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')

print("[Classical NN] Training complete.")

# --- Step 3: Extract features from Classical NN ---
with torch.no_grad():
    feats_cnn = model_classical(X_test_tensor, return_features=True).numpy()

# --- Step 4: PCA for Classical NN ---
pca_cnn = PCA(n_components=2)
reduced_cnn = pca_cnn.fit_transform(feats_cnn)

# --- Step 5: Plot Comparison (QNN vs Classical NN) ---
# First extract QNN features (reuse code from before)
with torch.no_grad():
    feats_qnn = model(X_test_tensor, return_features=True).numpy()

pca_qnn = PCA(n_components=2)
reduced_qnn = pca_qnn.fit_transform(feats_qnn)

# Plot both side by side
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].scatter(reduced_cnn[:, 0], reduced_cnn[:, 1], c=y_test, cmap='coolwarm', s=50, edgecolor='k')
axes[0].set_title("Classical NN Latent Space (PCA)")
axes[0].set_xlabel("PC 1")
axes[0].set_ylabel("PC 2")

axes[1].scatter(reduced_qnn[:, 0], reduced_qnn[:, 1], c=y_test, cmap='coolwarm', s=50, edgecolor='k')
axes[1].set_title("QNN Latent Space (PCA)")
axes[1].set_xlabel("PC 1")
axes[1].set_ylabel("PC 2")

plt.suptitle("Comparison: Classical NN vs Quantum NN Latent Spaces")
plt.show()